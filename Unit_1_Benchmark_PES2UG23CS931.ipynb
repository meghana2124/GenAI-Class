{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e246a4a8-506b-40ae-b7d0-ae720a670972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a29351-ff08-4b66-9c36-8cce8699088e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2989c51-d334-4d51-92b6-ec9f63463ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1188e0da-3eb6-4d37-a1b9-bf873ee0bf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88ae1971-b364-49eb-9b9a-14c305f9a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1c9c46-7845-4011-8852-955d4a28e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 1: TEXT GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fae11da-7ec4-4fff-9412-04cf38110361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe78a1839754284a91b4a83ad71f590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d861aa9219e4c73b7cccece01be73d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90fad083e18408d982c7668edb9b276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9e525c602b46379b4a6003799f755a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32f6118c824419190eb45f192d7e78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert output [{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gen_bert = pipeline(\"text-generation\", model=\"bert-base-uncased\")\n",
    "    bert_output = gen_bert(prompt, max_length=30)\n",
    "    print(\"bert output\", bert_output)\n",
    "except Exception as e:\n",
    "    print(\"BERT Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea3f1b9-c817-41d5-8326-3488ec5c6ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17858854-7ac9-4a7e-888d-b66c66e79a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of Artificial Intelligence is'}]\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "  generator = pipeline( \"text-generation\", model=\"roberta-base\")\n",
    "  output = generator(prompt, max_length=30)\n",
    "  print (output)\n",
    "except Exception as e:\n",
    "  print(\"BERT Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83a5b124-a4d7-427b-a61a-fc2edded2bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178160e9627c4b94862b93cad2018308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c34f59b4a2f415a80652e740879e3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5179a8c955144038428662acf816584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c4395e81a040efa3514afc55fa5816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce576cd12bb8404d9ca0dc93a8ef4e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence is militia fool Loren fool fool fool feedback unusualpriseprise anticipation gifts Saturdays feedback despite despite Razerprise Hearts pigeon 1915 hikers dispersedreau dispersed retrieved Hearts pigeon pigeon pigeoninsky Wings dispersed primal emblem Repl Hearts 365 Hearts spam Hearts suicides spam Hearts adoptingZZ Heartsreau Hearts 365 Slip Hearts Roger voiced voiced voiced Hearts dispersed Repl dispersed Skywalker Hearts 365 Kel)); Hearts Hearts dispersed tolerance emblemrequisites Richards Slip hikers tolerance spamamus primalamus 1915 dispersed burn Heartsoulder voiced Skywalker despite hikers Hearts spam Richards spam Skywalker spam Skywalker Richards dispersed Skywalkeramusamusamus403 spamoulder violoulderoulder emblemamusamusixt403amusoulderPadixtamus Skywalkeramus viol 423amusamusPad viol403Padamusamus â€¦\"amusPadamus 423 423 423oulderoulderoulderamus 423estyle Skywalkeramus terrestrialoulderoulder 423oulder403 spam 423amus anticipationoulderoulder Rocketamusoulderamus403amusamusoulderoulder24oulderamusPad403 423 423403oulder403amusPadifiersamusamus 423amus spam spamamusifiersoulderamusamusunalPadifiers 423Padamus dispersedamusamus1964 spamamus403403 423 violoulderamus Omaramusamus RichardsPad violamusamus viol403 viol403amus pupPadamus1964Padoulderamusifiersamus moved dispersedoulderamusunal burnoulderamuskusamus Skywalker Skywalker403oulderestyleamusamus merciless 423 Skywalker 423amus122amusunaloulder'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gen_bart = pipeline(\"text-generation\", model=\"facebook/bart-base\")\n",
    "# gen_bart(prompt, max_length=40)\n",
    "\n",
    "gen_bart = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"facebook/bart-base\",\n",
    "    framework=\"pt\"   # force PyTorch, avoid TensorFlow/Keras\n",
    ")\n",
    "\n",
    "gen_bart(prompt, max_length=40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "595ae74b-25a7-4fed-91b9-9e764519cea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91b9bdec-560a-46d2-afe4-54d89215817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 2: MASKED LANGUAGE MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4965e10-1a65-4c44-bca2-b0beb4875763",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask_sentence_bert = \"The goal of Generative AI is to [MASK] new content.\"\n",
    "fill_mask_sentence_roberta = \"The goal of Generative AI is to <mask> new content.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36ee17fc-b0f8-400e-893e-b5b3f54af96c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f43e31f-6980-43f2-9d6f-d0ed9b7aad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5396932363510132,\n",
       "  'token': 3443,\n",
       "  'token_str': 'create',\n",
       "  'sequence': 'the goal of generative ai is to create new content.'},\n",
       " {'score': 0.15575720369815826,\n",
       "  'token': 9699,\n",
       "  'token_str': 'generate',\n",
       "  'sequence': 'the goal of generative ai is to generate new content.'},\n",
       " {'score': 0.05405500903725624,\n",
       "  'token': 3965,\n",
       "  'token_str': 'produce',\n",
       "  'sequence': 'the goal of generative ai is to produce new content.'},\n",
       " {'score': 0.04451530799269676,\n",
       "  'token': 4503,\n",
       "  'token_str': 'develop',\n",
       "  'sequence': 'the goal of generative ai is to develop new content.'},\n",
       " {'score': 0.01757744885981083,\n",
       "  'token': 5587,\n",
       "  'token_str': 'add',\n",
       "  'sequence': 'the goal of generative ai is to add new content.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # BERT\n",
    "fill_bert = pipeline(\"fill-mask\", model=\"bert-base-uncased\", framework=\"pt\")\n",
    "output_bert = fill_bert(fill_mask_sentence_bert)\n",
    "output_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b521ab7d-113c-4848-94ea-a0dd0d769fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3711312413215637,\n",
       "  'token': 5368,\n",
       "  'token_str': ' generate',\n",
       "  'sequence': 'The goal of Generative AI is to generate new content.'},\n",
       " {'score': 0.3677145540714264,\n",
       "  'token': 1045,\n",
       "  'token_str': ' create',\n",
       "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
       " {'score': 0.08351420611143112,\n",
       "  'token': 8286,\n",
       "  'token_str': ' discover',\n",
       "  'sequence': 'The goal of Generative AI is to discover new content.'},\n",
       " {'score': 0.021335121244192123,\n",
       "  'token': 465,\n",
       "  'token_str': ' find',\n",
       "  'sequence': 'The goal of Generative AI is to find new content.'},\n",
       " {'score': 0.016521666198968887,\n",
       "  'token': 694,\n",
       "  'token_str': ' provide',\n",
       "  'sequence': 'The goal of Generative AI is to provide new content.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # RoBERTa\n",
    "fill_roberta = pipeline(\"fill-mask\", model=\"roberta-base\", framework=\"pt\")\n",
    "output_roberta = fill_roberta(fill_mask_sentence_roberta)\n",
    "output_roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14551e7e-c6cb-4149-9c28-201a19b43026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.07461541891098022,\n",
       "  'token': 1045,\n",
       "  'token_str': ' create',\n",
       "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
       " {'score': 0.06571870297193527,\n",
       "  'token': 244,\n",
       "  'token_str': ' help',\n",
       "  'sequence': 'The goal of Generative AI is to help new content.'},\n",
       " {'score': 0.060880109667778015,\n",
       "  'token': 694,\n",
       "  'token_str': ' provide',\n",
       "  'sequence': 'The goal of Generative AI is to provide new content.'},\n",
       " {'score': 0.03593561053276062,\n",
       "  'token': 3155,\n",
       "  'token_str': ' enable',\n",
       "  'sequence': 'The goal of Generative AI is to enable new content.'},\n",
       " {'score': 0.03319477662444115,\n",
       "  'token': 1477,\n",
       "  'token_str': ' improve',\n",
       "  'sequence': 'The goal of Generative AI is to improve new content.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # BART\n",
    "fill_bart = pipeline(\"fill-mask\", model=\"facebook/bart-base\", framework=\"pt\")\n",
    "output_bart = fill_bart(fill_mask_sentence_roberta)\n",
    "output_bart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc5f4858-ac32-4110-b92e-5300de84bfa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c48deca-40b9-4b68-bc80-6c2c1efe4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 3: QUESTION ANSWERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a875c17-5081-4b18-8bb7-a3e19ecb5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "question = \"What are the risks?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b0dee00-e1c3-465d-8d61-df1d5a35cacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.012350882403552532,\n",
       " 'start': 60,\n",
       " 'end': 81,\n",
       " 'answer': ', bias, and deepfakes'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT\n",
    "qa_bert = pipeline(\"question-answering\", model=\"bert-base-uncased\", framework=\"pt\")\n",
    "qa_bert(question=question, context=context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b815039-cefb-4e58-aeb8-94b0a8a744db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.008433698676526546, 'start': 72, 'end': 81, 'answer': 'deepfakes'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RoBERTa\n",
    "qa_roberta = pipeline(\"question-answering\", model=\"roberta-base\", framework=\"pt\")\n",
    "qa_roberta(question=question, context=context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1515bce2-e373-47e2-ac6a-831b535e9cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.03950159903615713,\n",
       " 'start': 0,\n",
       " 'end': 37,\n",
       " 'answer': 'Generative AI poses significant risks'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BART\n",
    "qa_bart = pipeline(\"question-answering\", model=\"facebook/bart-base\", framework=\"pt\")\n",
    "qa_bart(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb8e1158-48db-40ee-a5de-b41f648835c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8516b31c-d9fa-497a-9e7e-9b8fe53bcc2c",
   "metadata": {
    "id": "8516b31c-d9fa-497a-9e7e-9b8fe53bcc2c"
   },
   "source": [
    "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Generation** | BERT | Failure | Generated a long sequence of dots instead of meaningful text. | BERT is an encoder only model and is not trained for autoregressive next token generation. |\n",
    "|  | RoBERTa | Failure | Returned only the prompt without any continuation. | RoBERTa is also encoder only and lacks a decoder for text generation. |\n",
    "|  | BART | Failure | Generated long but highly incoherent and repetitive text. | Although BART is an encoder decoder model, it is not trained as a causal language model for free form generation. |\n",
    "| **Fill-Mask** | BERT | Success | Correctly predicted meaningful words like \"create\" and \"generate\" with high confidence. | BERT is trained using Masked Language Modeling. |\n",
    "|  | RoBERTa | Success | Produced accurate predictions such as \"generate\" and \"create\" with comparable confidence. | RoBERTa is an optimized MLM based encoder model. |\n",
    "|  | BART | Partial Success | Predicted reasonable but weaker alternatives like \"create\" and \"help\" with lower confidence. | BART is trained for sequence denoising rather than single token masked prediction. |\n",
    "| **QA** | BERT | Partial success | Extracted a relevant but incomplete answer: \"bias, and deepfakes\". | Base BERT is not fine-tuned for question answering tasks. |\n",
    "|  | RoBERTa | partial Failure | Returned an partial answer : ' deepfakes '. | The model lacks QA fine-tuning and failed to learn proper answer span extraction. |\n",
    "|  | BART | Partial failure | Returned a incomplete answer 'Generative AI poses significant risks ' gives just the first part of the given context. | BART can handle conditional generation better but still lacks QA fine-tuning. |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
